---
title: "Module 1.4 - Bootstrap and permutation"
author: "A. Gamble, I. Ali, N. Zaitlen, R. Wollman"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, echo = FALSE}

library(ggplot2)
library(tidyverse)
library(tidyr)
library(dplyr)

```

## Overview

This session will cover **bootstrapping** methods in statistics to calculate summary
statistics. We will use techniques from previous sessions in order to simulate experiment 
data collection given a set of assumptions. We will then use a bootsrapping approach to 
create summary statistics from this data.

Through the session we will increase the complexity of our bootsrapping examples
to show the scope of its uses and then share a permutation example which will
calculate a p-value under a null hypothesis.

## Simulating an experiment

This section mimics actual experimental data, this is *not* a statistical procedure. 
In this case, we will simulate an experiment where a researcher is observing the 
number of vesicles in a cell. We will set the number of observations made (number 
of cells we look at) in our simulated experiment. We will provide a 'true' mean number 
of vesicles/cell for the experiment. We will use rpois() to create the data set 
based on our defined parameters. We will convert the data set into a data frame and 
then calculate the average.

```{r simulatedata}

# Set the seed of the random number generator
set.seed(1)

# Number of observations aka how many cells are we going to look at?
n_obs <- 30

# This is our assumed true value
true_mean_vesicles_per_cell <- 2.3 

# Simulate the experiment with rpois()
vesicles_per_cells <- rpois(n_obs, true_mean_vesicles_per_cell)

# Convert the data set into a data frame which will be needed for plotting
myexperiment <- data.frame(vesicles_per_cells)

# Calculate average vesicles per cell
avg_v_per_cell <- mean(vesicles_per_cells)

```

We have now created a data set that contains the number of vesicles observed in
30 cells, we have converted it to the correct data structure for plotting and 
calculated the mean for the data set. 

Lets plot the data. 

Create a histogram looking at the number of vesicles per cell. In addition include
a line on top of the histogram to show the average of the simulated data set in red 
and the true mean vesicles per cell in blue. 

```{r simulateddata2}

# Pro tip: use geom_vline() to add a vertical line with a designated x intercept

# Add a subtitle your plot title with ggtitle("title", "subtitle")
ggplot(myexperiment, aes(x = vesicles_per_cells)) + 
  geom_histogram(binwidth = 1) + 
  geom_vline(xintercept = avg_v_per_cell, color = "red") + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") + 
  ggtitle("Vesicle per cell", "Red = estimate, Blue = truth") +
  labs(x = "Vesicles per Cell", 
       y = "Count")

```

Because this is a *simulated* experiment we can repeat it as many times as needed.
Let's pretend we had a large team that was able to repeat the experiment 25 times.

```{r createreplicates}

# Set the number of times we want to repeat the experiment (replicates)
replicates <- 25

# We can multiply our observations by our replicates in order to rapidly simulate
# the collection of the data.
vesicles_per_cells <- rpois(n_obs*replicates, true_mean_vesicles_per_cell)

# Create a factor to designate experiment number and bind to the data.frame
# use gl() to generate factor levels
experiment_id <- gl(replicates, n_obs)
myexperiment_replicates <- data.frame(vesicles = vesicles_per_cells, 
                                    replicate = experiment_id)

# Create a data frame of averages grouped by the 'replicate' factor we created above
df.avg <- myexperiment_replicates %>%
  group_by(replicate) %>%
  summarize(avg_per_replicate = mean(vesicles))

```

Let's take a look at the results of our team's experiments.

### Challenge 1

Plot all the experiment replicates using replicate number as the separating facet.

```{r challenge1}

ggplot(myexperiment_replicates, aes(vesicles_per_cells)) + 
  geom_histogram(binwidth = 1) + 
  facet_wrap(~replicate) + 
  geom_vline(data = df.avg, aes(xintercept = avg_per_replicate), color = "red") + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") +
  ggtitle("Vesicle per cell", "Red = estimate, Blue = truth") +
  labs(x = "Vesicles Per Cell", 
       y = "Count")

```

Plot all experimental averages in a histogram with the true mean indicated with a blue
line on the plot.

```{r meansdistribution}

ggplot(df.avg, aes(avg_per_replicate)) + 
  geom_histogram(binwidth = 0.1) + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") +
  ggtitle("Distribution of sample mean", "True Mean is in Blue") +
  labs(x = "Mean Vesicles per replicate", 
       y = "Count")

```

## Intro to Bootstrapping

In the last example, we simulated repeating the experiment 25 times, what happens if we can't? 
In many cases we will only be able to conduct the experiment once and have a limited data set.
Bootstrapping allows us to **create experimental sub-samples** by sampling from the sample! This
can help us estimate summary statistics and give us more confidence in our final results.

Essentially, bootstrapping is the process of creating a large number of sub-samples from your
original data set by selecting values **with replacement**. While each sub-sample is created from
the original data set, it does not look exactly the same. We can calculate the means and variance
from the sub-sampled data sets to generate an estimate of the range of possible means in the data
set.

```{r subsampling}

# Create a sub-sample with the sample() command
# Using our previously created  data set, lets create a sample of vesicles per 
# cell that is sub-sampled using bootstrapping with the same number of 
# observations and replicates that we used before
# Since bootstrapping is sampling done with replacement be sure to set 
# replace = TRUE
subsample1 <- sample(myexperiment$vesicles_per_cells, n_obs*replicates, replace = TRUE)

# The data frame using the sub-sampled vesicles data and experiment_id from before
subsample_boostraps <- data.frame(vesicles = subsample1, replicate = experiment_id)

# Calculate means for the bootstrapped sub-samples
bootstraps_averages <- subsample_boostraps %>%
                    group_by(replicate) %>%
                    summarise(mean_vesicles = mean(vesicles))

# Using the same plotting approach from above lets visualize the 25 sub-samples
# obtained by bootstrapping
ggplot(subsample_boostraps, aes(vesicles)) + 
  geom_histogram(binwidth = 1) + 
  facet_wrap(~replicate) + 
  geom_vline(data = bootstraps_averages, aes(xintercept = mean_vesicles), color = "red") + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") 

```

Look at the distribution of our bootstrap estimate of the mean using the method from earlier.

```{r bootstrapdistribution}

ggplot(bootstraps_averages, aes(x = mean_vesicles)) + 
  geom_histogram(bins = 15) + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") + 
  ggtitle("Bootstrap distribution of the mean")

```

In practice we do not need to keep or visualize all the samples of samples, we just
need the summary statistics. For this example let's scale up by creating an array
of 1000 sub samples using a for loop and visualizing the results.

```{r scalingup}

# Create an empty array which can be filled with the 1000 sub samples
avg_boot_sample <- array(NA, 1000)

# Create a for loop that collects the sub-samples and fills the array
for (i in 1:length(avg_boot_sample)){
  bootsample <- sample(vesicles_per_cells, replace = TRUE)
  avg_boot_sample[i] <- mean(bootsample)
}

# Calculate the summary statistics from your array of sub-samples
mean(avg_boot_sample)
sd(avg_boot_sample)
quantile(avg_boot_sample, c(0.025, 0.975))

# Let's take a look
# Notice ggplot can only work with data frames
# In this case we can convert the avg_boot_sample array into a 
# data frame within the ggplot command
ggplot(data.frame(avg_boot_sample) , aes(avg_boot_sample)) + 
  geom_histogram() + 
  geom_vline(xintercept = avg_v_per_cell, color = "red") + 
  geom_vline(xintercept = mean(avg_boot_sample), color = "green") + 
  geom_vline(xintercept = true_mean_vesicles_per_cell, color = "blue") + 
  ggtitle("Vesicle per cell", "Red = estimate, Blue = truth, Green = bootstrap")

```

## Considering 2 variables: Permutation

References: 
Liu et al. 2020. NeuroImage. https://doi.org/10.1016/j.neuroimage.2020.116879

Blink-related oscillations are a recently discovered neurophysiological 
response associated with spontaneous blinking, distinct from the well-known 
oculomotor and visual suppression effects. In the following example we are
going to simulate a data set based on neuron spike signals and eye blinking.

This is not real data and should not reflect real data, but if you would like
to learn more about blinking and neuronal activity check out the paper linked above!


```{r setupdata, echo = TRUE}

# We can start by simulating our data set

# In this data set, we consider two variables: eye blinking and neuron 
# spike rates. We will then explore if these two variables are correlated.

n_obs <- 20; 

# If neuron spike rate is normally distributed with a mean rate of 15
# spikes per minute and eye blinking can occur before or after the neuron
# spike, lets design a data set that explores this association
set.seed(2)
neuron.spike.rate <- rnorm(n_obs, mean = 15)
eye.blinking <- neuron.spike.rate + rnorm(n_obs, 0, 2)
myexperiment <- data.frame(neuron.spike.rate = neuron.spike.rate, 
                           eye.blinking = eye.blinking)

# Have a look at the simulated data. As a table this isn't so useful.
# Lets use the skills we learned in previous sessions to explore this
# data set.
View(myexperiment)

```

### Challenge 2

Explore the data by plotting. We want to be able to see the individual
data points and also a visual linear model association (recall geom_smooth()). 
Include descriptive labels including a title, subtitle, and axis names.

```{r challenge2, echo = TRUE}
# Plot the data on a scatter plot, include a title and axis labels
ggplot(myexperiment, aes(x = neuron.spike.rate, y = eye.blinking)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  labs(title = "Exploring Associations",
       subtitle = "Eye Blinks and Neuron Spike Rate",
       x = "Neuron Spike Rate per Minute",
       y = "Blinks per Minute")
```

## Intro to Linear Models

Linear models can help assess correlations and other characteristics when
comparing two variables side-by-side. In the example above, we used `ggplot()`
to create a visual linear model. Using R we can quickly calculate many
characteristics related to our linear models.

```{r linearmodel}

# When constructing a linear model in R, you can use the 'lm()' command
# if you assign lm() to an object you can see all the information it collects
# from your model. 

testmodel <- lm(myexperiment$eye.blinking ~ myexperiment$neuron.spike.rate)

testmodel

# notice that our testmodel object is a list that contains 12 components
# including coefficients (coef), residuals and a variety of other items
# if you a assign a lm() to an object you can extract components of the 
# model for reports. 

testmodel$coefficients

```

### Challenge 3: Linear Model Functions

Create a function that returns the slope of the linear model.

```{r challenge3, echo = TRUE}

# Write a function that returns the slope of a y ~ x linear regression
myslope <- function(x, y) {
  model <- lm(y ~ x) # lm creates a list containing model characteristics
  slope <- model$coefficients[[2]] # the slope value is the 2nd item listed under coef
  return(slope)
}

# Return the slope of the association between neuron spike rate and eye blinking
real_slope <- myslope(neuron.spike.rate, eye.blinking)
real_slope

```

## Permutation

**Permutation** can be used the explore the association between two variables. 
Permutation consists in the re-arrangement of all or part of a data set. Let's 
consider two variables x and y. If those two variables are correlated, then a 
given value of x should be more likely associated to a given value of y. Permuting 
y independently of x, would thus impact the observed association between x and y. 
However, if x and y are not independent, permutation would have no impact on the 
observed association. 

Permutation is very similar to bootstrapping, but happens **without replacement**.
Importantly, if the variables are not correlated, it wouldn't matter if we shuffle
one of them. If the variables are correlated, the association would break.


```{r slope_after_permutation, echo = TRUE}

# create a permutation index sample, and a new object with the values rearranged
perm_indx <- sample(n_obs, replace = FALSE) # new index
perm_blink <- eye.blinking[perm_indx]; # rearranged data

# Add to data frame with an indicator
indicator <- as.factor(c(array("perm", n_obs), array("real", n_obs)))
neuron <- c(neuron.spike.rate, neuron.spike.rate) # order does not change
blink <- c(perm_blink, eye.blinking) # order changes
myexperiment <- data.frame(indicator, neuron, blink) # assemble data frame


# Plot the data
ggplot(myexperiment, aes(x = neuron, y = blink, color = indicator)) + 
  geom_point(aes(color = indicator)) +
  geom_smooth(method = lm) + 
  labs(title = "Comparing Permuted and Original Dataset",
       x = "Neuronal Spikes per Minute",
       y = "Blinks per Minute")

```

Depending on the results of your test, the results may be dramatic or subtle.
Similar to bootstrapping, permutation testing is often scaled up dramatically
and summarized. This would essentially give us a level of confidence that our
result is real based on the results of repeated permutation testing.

Using a similar approach to our bootstrapping test, repeat the permutation test
1000 times to get a distribution of possible slopes assuming no connection.

```{r permutation_atscale}

perm_slopes <-  array(NA, dim = 1000)

for (i in 1:length(perm_slopes)) {
  perm_blink <- eye.blinking[sample(n_obs)];
  perm_slopes[i] <- myslope(neuron.spike.rate, perm_blink)
}

```

### Challenge 4

Create a plot showing the distribution of permuted slopes to see how
closely it aligns with the original slope.

```{r challenge4}

ggplot(data.frame(perm_slopes), aes(perm_slopes)) + 
  geom_histogram() + 
  geom_vline(xintercept = real_slope)

```

## Permutation p-values

We can calculate a p-value which corresponds to the probability 
of observing a difference between the original slope and the 
permuted slope samples. If we are testing the assumption that 
the two variables are NOT associated, then we subtract that value
from 1 to obtain our p-value

```{r permute_pval}

p_value <- 1 - mean(real_slope > perm_slopes)

# notice what we are calculating here: mean(real_slope > perm_slopes)
# in this case we are calculating the avg where TRUE = 1 and FALSE = 0
# this returns a probability where the closer we are to 1 the more
# frequently the permutation test "broke" the association. 

p_value

```
 

 
